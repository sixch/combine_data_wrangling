	Every February, 300 of the best college football players are invited to the NFL Scouting Combine in Indianapolis. The NFL Scouting Combine is a four-day event which includes many different player attributes in the hopes of the next great player. Some attributes that are measured are height, weight, speed, agility, leaping ability, strength, intelligence, background check, interviews and many more. 
	The goal of this project is to make a predictive model that can classify which round (categorical variable) a player will get drafted, or possibly not even get drafted, by an NFL team according to players position and the numerical attributes that were measured during the NFL Scouting Combine. The NFL draft has seven rounds, with each round having 32 selections. There are 32 teams in the NFL and each team gets one draft pick per round, assuming they didn’t trade the pick away to another team. This model will be able to predict if a player is drafted in rounds 1-7 or not drafted at all. Also, if a player is the very first pick in the NFL draft in round one (pick 1.01), that is much more siginificant (make more money), than being the last pick in round one (pick 1.32). This model will not only predict what round a player is drafted, but also the exact pick total. For instance, if a player is the first player drafted in the NFL draft, (pick 1.01), they would be pick total 1. If a player is the last player selected in round 2, (pick 2.32), they would be pick total 64. In order to predict the pick total, would use a regression model for that, since we are predicting a numerical value. Pick total is probably only really important to players and their agents during round 1. After round 1, knowing the exact pick total isn’t as critical and usually just want to know which round they will get selected.
	This model is not for one specific client, but for all college football players, NFL coaches, general managers and player agents. People that would get value out of this predictive model would be college football players that are training for the NFL Scouting combine. While training, they could enter their position (categorical variable) along with their current numerical attributes into the model, to get an idea of when they will be drafted or if drafted at all. They could also tinker around with their numerical attributes and see when they would get drafted if they gained five pounds or maybe decreased their forty-yard dash time by a tenth of a second or bench pressed 225 lbs for a few more repetitions, etc. For most positions, NFL draft enthusiast believe that the forty-yard dash time carries the most weight to determine when or if a player is drafted. This model will give a good idea of which attributes carry the most weight for each position. For the elite football recruits, the model would be valuable so they would know if they are a top 10 overall pick or maybe they will be around the end of the first round or the beginning of the second round. NFL coaches and general manager could find this model valuable, so they have a better idea of when they should draft a player. If the model predicts a player should be drafted in round 5, by knowing that information is very helpful, so a team doesn’t draft a player too early, like round 3 or 4 and can wait until round 5. Player agents may find the model valuable so they have an idea of when a player will be drafted, so they know which player they want to have as a client. Obviously, player data that is collected from the NFL Scouting Combine aren’t the sole factors in determining NFL draft outcome, but this model would be a useful tool so players while training have an idea on what athletic tests they really need to focus or improve on. 
	Combine.csv dataset had every player that was invited to the NFL Scouting Combine from 1999 to 2015. Almost 5000 players were invited to the NFL Scouting Combine, during this time span. After importing the combine.csv dataset, I had to drop some insignificant/redundant columns from the dataframe. The dataframe had a lot of 0.0 values, so I first checked to see which columns had a high percentage of 0.0 values and dropped those columns from the dataframe. In the position column, I dropped rows with values P, K, and LS, since I have no interest in analyzing those positions. Position OC and C, both mean position center, so I replaced OC to be C, therefore creating centers to only be in class C. I had to do the same with position NT and DT, replace NT as DT, so defensive tackles are only classified as DT. Safeties were classified as FS and SS, I replaced both to be classified only as S. College Safeties usually aren’t classified as a FS or SS, until after they are drafted by their respective team. Replaced all 0 values with NaN, so calculations will exclude NaN values. Some players in the dataframe had the same name, so had to remove duplicate player names by assigning _dp to the duplicate player name. For instance, there were two players named Brandon Moore, after removing duplicates, there is now one player named Brandon Moore and the other Brandon Moore_dp. One issue the combine.csv dataset is that it had no information of when or if a player was drafted in 2015. At this point, this dataframe is labeled nflnan.
	Luckily, I found another dataset, nfl_draft.csv, and this dataset only had players that were drafted, but this will work for my missing round data for my 2015 players. Any players that weren’t drafted in 2015 will still be nan, but that is fine. In order to accomplish this task, I had to import this dataset and do a left merge with nflnan. Nfl_draft.csv dataset is labeled, draft2, in the notebook. Now, nflnan has players that were drafted AND undrafted by NFL teams, so will have more players than dataframe draft2, since draft2 ONLY has players that were drafted, hence the left join with nflnan. Prior to the merge, I had to make sure there were no duplicate names in dataframe draft2 and replace duplicates with _dp at the end of their name. I had to make sure there were no duplicate names in both nflnan and draft2 dataframes, in order to have the same number of rows as nflnan, after doing the merge. I used players name and year for the keys of the merge, which is another reason I couldn’t have duplicate names. nflnan had 4924 rows, draft2 had 4332 rows and after merging both, it had 4924 rows, success! The merge dataframe is labeled, newdf. The newdf dataframe now has round information for players in year 2015 that were drafted, success! Subsequently, I dropped column year and other insignificant columns. I still have plenty of NaN values and plan on dealing with those with some machine learning algorithms on a later date. I explored players height, weight, fortyyd columns with box-plots, to get an idea of the different percentiles for those positions and to check for outliers. I did a value_counts of every position to see which positions were most popular to be invited to the NFL Scouting Combine. Created a histogram of the weight column which revealed players weight to be bimodal, which isn’t surprising, since players in the NFL are usually either small and fast or big and slow. I would like to explore that bimodal distribution further, during my final project submission. Created a swarmplot of the weight of the positions, just to see how it looked. Will probably delete the swarmplot, as you can see the data gets all bunched together, hence the creation of the Empirical Cumulative Distribution Function (ecdf). I just love this function and how it shows distribution, will talk about this function more in just a bit. Got the descriptive statistics of the dataframes numerical columns, which in away, isn’t very helpful since players are very different, according to the position they play. For instance, players that play position OT, C, OG, DT are very big people, so they will have a much slower mean fortyyd time, than compared to players that play position WR, CB, RB, who are much smaller and faster. In order to backup that statement, I checked the correlation of weight and fortytime which as I suspected is strongly positive correlated at 0.87. I also created a correlation heatmap for all columns. Next, I did a hypothesis test to see if position CB and WR had the same mean fortyyd time. First plotted the CB and WR fortyyd time distribution, which showed both to have nearly identical distributions. Since CB and WR both have more than 30 observations, I calculated a z-score to get a p-value. The p-value was less than 0.05 which caused me to reject my null hypothesis that CB mean fortyyd time equals WR mean fortyyd time.
	Next, will create a classification and regression model to predict what round a player gets drafted or unfortunately doesn’t get drafted and if they do get drafted, the exact pick total. 

